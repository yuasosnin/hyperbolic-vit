
@article{clauset_hierarchical_2008,
	title = {Hierarchical structure and the prediction of missing links in networks},
	volume = {453},
	copyright = {2008 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature06830},
	doi = {10.1038/nature06830},
	abstract = {Networks are now a ubiquitous tool for representing the structure of complex systems, including the Internet, social networks, food webs, and protein and genetic networks. Unfortunately, the data describing these networks are in many cases incomplete or biased. A new study provides a general technique to divide network vertices into groups and sub-groups. Revealing such underlying hierarchies makes it possible to predict missing links from partial data with higher accuracy than previous methods.},
	language = {en},
	number = {7191},
	urldate = {2023-06-19},
	journal = {Nature},
	author = {Clauset, Aaron and Moore, Cristopher and Newman, M. E. J.},
	month = may,
	year = {2008},
	keywords = {Humanities and Social Sciences, Science, multidisciplinary},
	pages = {98--101},
}

@article{krioukov_hyperbolic_2010,
	title = {Hyperbolic geometry of complex networks},
	volume = {82},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.82.036106},
	doi = {10.1103/PhysRevE.82.036106},
	abstract = {We develop a geometric framework to study the structure and function of complex networks. We assume that hyperbolic geometry underlies these networks, and we show that with this assumption, heterogeneous degree distributions and strong clustering in complex networks emerge naturally as simple reflections of the negative curvature and metric property of the underlying hyperbolic geometry. Conversely, we show that if a network has some metric structure, and if the network degree distribution is heterogeneous, then the network has an effective hyperbolic geometry underneath. We then establish a mapping between our geometric framework and statistical mechanics of complex networks. This mapping interprets edges in a network as noninteracting fermions whose energies are hyperbolic distances between nodes, while the auxiliary fields coupled to edges are linear functions of these energies or distances. The geometric network ensemble subsumes the standard configuration model and classical random graphs as two limiting cases with degenerate geometric structures. Finally, we show that targeted transport processes without global topology knowledge, made possible by our geometric framework, are maximally efficient, according to all efficiency measures, in networks with strongest heterogeneity and clustering, and that this efficiency is remarkably robust with respect to even catastrophic disturbances and damages to the network structure.},
	number = {3},
	urldate = {2023-06-19},
	journal = {Physical Review E},
	author = {Krioukov, Dmitri and Papadopoulos, Fragkiskos and Kitsak, Maksim and Vahdat, Amin and Boguñá, Marián},
	month = sep,
	year = {2010},
	pages = {036106},
}

@misc{tifrea_poincare_2018,
	title = {Poincar\'e {GloVe}: {Hyperbolic} {Word} {Embeddings}},
	shorttitle = {Poincar\'e {GloVe}},
	url = {http://arxiv.org/abs/1810.06546},
	doi = {10.48550/arXiv.1810.06546},
	abstract = {Words are not created equal. In fact, they form an aristocratic graph with a latent hierarchical structure that the next generation of unsupervised learned word embeddings should reveal. In this paper, justified by the notion of delta-hyperbolicity or tree-likeliness of a space, we propose to embed words in a Cartesian product of hyperbolic spaces which we theoretically connect to the Gaussian word embeddings and their Fisher geometry. This connection allows us to introduce a novel principled hypernymy score for word embeddings. Moreover, we adapt the well-known Glove algorithm to learn unsupervised word embeddings in this type of Riemannian manifolds. We further explain how to solve the analogy task using the Riemannian parallel transport that generalizes vector arithmetics to this new type of geometry. Empirically, based on extensive experiments, we prove that our embeddings, trained unsupervised, are the first to simultaneously outperform strong and popular baselines on the tasks of similarity, analogy and hypernymy detection. In particular, for word hypernymy, we obtain new state-of-the-art on fully unsupervised WBLESS classification accuracy.},
	urldate = {2023-06-19},
	publisher = {arXiv},
	author = {Tifrea, Alexandru and Bécigneul, Gary and Ganea, Octavian-Eugen},
	month = nov,
	year = {2018},
	note = {arXiv:1810.06546 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{sarkar_low_2012,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Low {Distortion} {Delaunay} {Embedding} of {Trees} in {Hyperbolic} {Plane}},
	isbn = {9783642258787},
	doi = {10.1007/978-3-642-25878-7_34},
	abstract = {This paper considers the problem of embedding trees into the hyperbolic plane. We show that any tree can be realized as the Delaunay graph of its embedded vertices. Particularly, a weighted tree can be embedded such that the weight on each edge is realized as the hyperbolic distance between its embedded vertices. Thus the embedding preserves the metric information of the tree along with its topology. The distance distortion between non adjacent vertices can be made arbitrarily small – less than a (1 + ε) factor for any given ε. Existing results on low distortion of embedding discrete metrics into trees carry over to hyperbolic metric through this result. The Delaunay character implies useful properties such as guaranteed greedy routing and realization as minimum spanning trees.},
	language = {en},
	booktitle = {Graph {Drawing}},
	publisher = {Springer},
	author = {Sarkar, Rik},
	editor = {van Kreveld, Marc and Speckmann, Bettina},
	year = {2012},
	keywords = {Hyperbolic Plane, Minimum Span Tree, Voronoi Cell, Voronoi Diagram, Weighted Tree},
	pages = {355--366},
}

@article{huang_hyperbolic_2023,
	title = {Hyperbolic {Music} {Transformer} for {Structured} {Music} {Generation}},
	volume = {11},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2023.3257381},
	abstract = {In the field of music generation, generating structured music is a highly challenging research topic. Music generation methods are currently learned in Euclidean space and usually modeled as a time series without structural properties, but due to the limitations of the time series representation in Euclidean space, the hierarchical structure of music is difficult to learn, and the generated music is poorly structured. Therefore, based on hyperbolic theory, this paper proposes a Hyperbolic Music Transformer model, which considers the hierarchy in music and models the structured components of music in hyperbolic space. Meanwhile, in order for the network to have sufficient capacity to learn music data with hierarchical and power regular structure, a hyperbolic attention mechanism is proposed, which is an extension of the attention mechanism in hyperbolic space based on the definition of hyperboloid and Klein model. Subjective and objective experiments show that the model proposed in this paper is able to generate high-quality music with structure.},
	journal = {IEEE Access},
	author = {Huang, Wenkai and Yu, Yujia and Xu, Haizhou and Su, Zhiwen and Wu, Yu},
	year = {2023},
	keywords = {Data models, Electronic music, Music information retrieveal, Recurrent neural networks, Solid modeling, Structured music generation, Task analysis, Transformers, hyperbolic attention, hyperbolic music transformer, hyperbolic theory},
	pages = {26893--26905},
}

@misc{lopez_fully_2020,
	title = {A {Fully} {Hyperbolic} {Neural} {Model} for {Hierarchical} {Multi}-{Class} {Classification}},
	url = {http://arxiv.org/abs/2010.02053},
	doi = {10.48550/arXiv.2010.02053},
	abstract = {Label inventories for fine-grained entity typing have grown in size and complexity. Nonetheless, they exhibit a hierarchical structure. Hyperbolic spaces offer a mathematically appealing approach for learning hierarchical representations of symbolic data. However, it is not clear how to integrate hyperbolic components into downstream tasks. This is the first work that proposes a fully hyperbolic model for multi-class multi-label classification, which performs all operations in hyperbolic space. We evaluate the proposed model on two challenging datasets and compare to different baselines that operate under Euclidean assumptions. Our hyperbolic model infers the latent hierarchy from the class distribution, captures implicit hyponymic relations in the inventory, and shows performance on par with state-of-the-art methods on fine-grained classification with remarkable reduction of the parameter size. A thorough analysis sheds light on the impact of each component in the final prediction and showcases its ease of integration with Euclidean layers.},
	urldate = {2023-06-19},
	publisher = {arXiv},
	author = {López, Federico and Strube, Michael},
	month = oct,
	year = {2020},
	note = {arXiv:2010.02053 [cs]},
	keywords = {Computer Science - Computation and Language, I.2.7},
}

@misc{ba_layer_2016,
	title = {Layer {Normalization}},
	url = {http://arxiv.org/abs/1607.06450},
	doi = {10.48550/arXiv.1607.06450},
	abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
	urldate = {2023-06-19},
	publisher = {arXiv},
	author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
	month = jul,
	year = {2016},
	note = {arXiv:1607.06450 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{nickel_learning_2018,
	title = {Learning {Continuous} {Hierarchies} in the {Lorentz} {Model} of {Hyperbolic} {Geometry}},
	url = {https://proceedings.mlr.press/v80/nickel18a.html},
	abstract = {We are concerned with the discovery of hierarchical relationships from large-scale unstructured similarity scores. For this purpose, we study different models of hyperbolic space and find that learning embeddings in the Lorentz model is substantially more efficient than in the Poincar\{é\}-ball model. We show that the proposed approach allows us to learn high-quality embeddings of large taxonomies which yield improvements over Poincar\{é\} embeddings, especially in low dimensions. Lastly, we apply our model to discover hierarchies in two real-world datasets: we show that an embedding in hyperbolic space can reveal important aspects of a company’s organizational structure as well as reveal historical relationships between language families.},
	language = {en},
	urldate = {2023-06-19},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Nickel, Maximillian and Kiela, Douwe},
	month = jul,
	year = {2018},
	pages = {3779--3788},
}

@misc{chen_fully_2022,
	title = {Fully {Hyperbolic} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2105.14686},
	doi = {10.48550/arXiv.2105.14686},
	abstract = {Hyperbolic neural networks have shown great potential for modeling complex data. However, existing hyperbolic networks are not completely hyperbolic, as they encode features in a hyperbolic space yet formalize most of their operations in the tangent space (a Euclidean subspace) at the origin of the hyperbolic space. This hybrid method greatly limits the modeling ability of networks. In this paper, we propose a fully hyperbolic framework to build hyperbolic networks based on the Lorentz model by adapting the Lorentz transformations (including boost and rotation) to formalize essential operations of neural networks. Moreover, we also prove that linear transformation in tangent spaces used by existing hyperbolic networks is a relaxation of the Lorentz rotation and does not include the boost, implicitly limiting the capabilities of existing hyperbolic networks. The experimental results on four NLP tasks show that our method has better performance for building both shallow and deep networks. Our code will be released to facilitate follow-up research.},
	urldate = {2023-06-19},
	publisher = {arXiv},
	author = {Chen, Weize and Han, Xu and Lin, Yankai and Zhao, Hexu and Liu, Zhiyuan and Li, Peng and Sun, Maosong and Zhou, Jie},
	month = mar,
	year = {2022},
	note = {arXiv:2105.14686 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{loshchilov_decoupled_2019,
	title = {Decoupled {Weight} {Decay} {Regularization}},
	url = {http://arxiv.org/abs/1711.05101},
	doi = {10.48550/arXiv.1711.05101},
	abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
	urldate = {2023-06-19},
	publisher = {arXiv},
	author = {Loshchilov, Ilya and Hutter, Frank},
	month = jan,
	year = {2019},
	note = {arXiv:1711.05101 [cs, math]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control},
}

@inproceedings{deng_imagenet_2009,
	title = {{ImageNet}: {A} large-scale hierarchical image database},
	shorttitle = {{ImageNet}},
	doi = {10.1109/CVPR.2009.5206848},
	abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500–1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
	booktitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	month = jun,
	year = {2009},
	note = {ISSN: 1063-6919},
	keywords = {Explosions, Image databases, Image retrieval, Information retrieval, Internet, Large-scale systems, Multimedia databases, Ontologies, Robustness, Spine},
	pages = {248--255},
}

@book{ungar_gyrovector_2009,
	address = {Cham},
	series = {Synthesis {Lectures} on {Mathematics} \& {Statistics}},
	title = {A {Gyrovector} {Space} {Approach} to {Hyperbolic} {Geometry}},
	isbn = {9783031012686 9783031023965},
	url = {https://link.springer.com/10.1007/978-3-031-02396-5},
	language = {en},
	urldate = {2023-06-19},
	publisher = {Springer International Publishing},
	author = {Ungar, Abraham Albert},
	year = {2009},
	doi = {10.1007/978-3-031-02396-5},
}

@book{ungar_analytic_2008,
	title = {Analytic {Hyperbolic} {Geometry} and {Albert} {Einstein}'s {Special} {Theory} of {Relativity}},
	isbn = {9789812772299 9789812772305},
	url = {http://www.worldscientific.com/worldscibooks/10.1142/6625},
	language = {en},
	urldate = {2023-06-19},
	publisher = {WORLD SCIENTIFIC},
	author = {Ungar, Abraham Albert},
	month = feb,
	year = {2008},
	doi = {10.1142/6625},
}

@misc{yue_hyperbolic_2023,
	title = {Hyperbolic {Contrastive} {Learning}},
	url = {http://arxiv.org/abs/2302.01409},
	doi = {10.48550/arXiv.2302.01409},
	abstract = {Learning good image representations that are beneficial to downstream tasks is a challenging task in computer vision. As such, a wide variety of self-supervised learning approaches have been proposed. Among them, contrastive learning has shown competitive performance on several benchmark datasets. The embeddings of contrastive learning are arranged on a hypersphere that results in using the inner (dot) product as a distance measurement in Euclidean space. However, the underlying structure of many scientific fields like social networks, brain imaging, and computer graphics data exhibit highly non-Euclidean latent geometry. We propose a novel contrastive learning framework to learn semantic relationships in the hyperbolic space. Hyperbolic space is a continuous version of trees that naturally owns the ability to model hierarchical structures and is thus beneficial for efficient contrastive representation learning. We also extend the proposed Hyperbolic Contrastive Learning (HCL) to the supervised domain and studied the adversarial robustness of HCL. The comprehensive experiments show that our proposed method achieves better results on self-supervised pretraining, supervised classification, and higher robust accuracy than baseline methods.},
	urldate = {2023-06-19},
	publisher = {arXiv},
	author = {Yue, Yun and Lin, Fangzhou and Yamada, Kazunori D. and Zhang, Ziming},
	month = feb,
	year = {2023},
	note = {arXiv:2302.01409 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{bdeir_hyperbolic_2023,
	title = {Hyperbolic {Geometry} in {Computer} {Vision}: {A} {Novel} {Framework} for {Convolutional} {Neural} {Networks}},
	shorttitle = {Hyperbolic {Geometry} in {Computer} {Vision}},
	url = {http://arxiv.org/abs/2303.15919},
	doi = {10.48550/arXiv.2303.15919},
	abstract = {Real-world visual data exhibit intrinsic hierarchical structures that can be represented effectively in hyperbolic spaces. Hyperbolic neural networks (HNNs) are a promising approach for learning feature representations in such spaces. However, current HNNs in computer vision rely on Euclidean backbones and only project features to the hyperbolic space in the task heads, limiting their ability to fully leverage the benefits of hyperbolic geometry. To address this, we present HCNN, the first fully hyperbolic convolutional neural network (CNN) designed for computer vision tasks. Based on the Lorentz model, we generalize fundamental components of CNNs and propose novel formulations of the convolutional layer, batch normalization, and multinomial logistic regression. Experimentation on standard vision tasks demonstrates the superiority of our HCNN framework and the Lorentz model in both hybrid and fully hyperbolic settings. Overall, we believe our contributions provide a foundation for developing more powerful HNNs that can better represent complex structures found in image data. Our code is publicly available at https://github.com/kschwethelm/HyperbolicCV.},
	urldate = {2023-06-19},
	publisher = {arXiv},
	author = {Bdeir, Ahmad and Schwethelm, Kristian and Landwehr, Niels},
	month = may,
	year = {2023},
	note = {arXiv:2303.15919 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{lensink_fully_2022,
	title = {Fully hyperbolic convolutional neural networks},
	volume = {9},
	issn = {2197-9847},
	url = {https://doi.org/10.1007/s40687-022-00343-1},
	doi = {10.1007/s40687-022-00343-1},
	abstract = {Convolutional neural networks (CNN) have recently seen tremendous success in various computer vision tasks. However, their application to problems with high dimensional input and output, such as high-resolution image and video segmentation or 3D medical imaging, has been limited by various factors. Primarily, in the training stage, it is necessary to store network activations for back-propagation. In these settings, the memory requirements associated with storing activations can exceed what is feasible with current hardware, especially for problems in 3D. Motivated by the propagation of signals over physical networks, that are governed by the hyperbolic Telegraph equation, in this work we introduce a fully conservative hyperbolic network for problems with high-dimensional input and output. We introduce a coarsening operation that allows completely reversible CNNs by using a learnable discrete wavelet transform and its inverse to both coarsen and interpolate the network state and change the number of channels. We show that fully reversible networks are able to achieve results comparable to the state of the art in 4D time-lapse hyper-spectral image segmentation and full 3D video segmentation, with a much lower memory footprint that is a constant independent of the network depth. We also extend the use of such networks to variational auto-encoders, where optimization begins from an exact recovery and we discover the level of compression through optimization.},
	language = {en},
	number = {4},
	urldate = {2023-06-19},
	journal = {Research in the Mathematical Sciences},
	author = {Lensink, Keegan and Peters, Bas and Haber, Eldad},
	month = sep,
	year = {2022},
	keywords = {Auto-encoders, Hyperbolic PDE, Neural networks, Wavelets},
	pages = {60},
}

@misc{micic_hyperbolic_2018,
	title = {Hyperbolic {Deep} {Learning} for {Chinese} {Natural} {Language} {Understanding}},
	url = {http://arxiv.org/abs/1812.10408},
	doi = {10.48550/arXiv.1812.10408},
	abstract = {Recently hyperbolic geometry has proven to be effective in building embeddings that encode hierarchical and entailment information. This makes it particularly suited to modelling the complex asymmetrical relationships between Chinese characters and words. In this paper we first train a large scale hyperboloid skip-gram model on a Chinese corpus, then apply the character embeddings to a downstream hyperbolic Transformer model derived from the principles of gyrovector space for Poincare disk model. In our experiments the character-based Transformer outperformed its word-based Euclidean equivalent. To the best of our knowledge, this is the first time in Chinese NLP that a character-based model outperformed its word-based counterpart, allowing the circumvention of the challenging and domain-dependent task of Chinese Word Segmentation (CWS).},
	urldate = {2023-06-19},
	publisher = {arXiv},
	author = {Micic, Marko Valentin and Chu, Hugo},
	month = dec,
	year = {2018},
	note = {arXiv:1812.10408 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{krause_3d_2013,
	title = {{3D} {Object} {Representations} for {Fine}-{Grained} {Categorization}},
	doi = {10.1109/ICCVW.2013.77},
	abstract = {While 3D object representations are being revived in the context of multi-view object class detection and scene understanding, they have not yet attained wide-spread use in fine-grained categorization. State-of-the-art approaches achieve remarkable performance when training data is plentiful, but they are typically tied to flat, 2D representations that model objects as a collection of unconnected views, limiting their ability to generalize across viewpoints. In this paper, we therefore lift two state-of-the-art 2D object representations to 3D, on the level of both local feature appearance and location. In extensive experiments on existing and newly proposed datasets, we show our 3D object representations outperform their state-of-the-art 2D counterparts for fine-grained categorization and demonstrate their efficacy for estimating 3D geometry from images via ultra-wide baseline matching and 3D reconstruction.},
	booktitle = {2013 {IEEE} {International} {Conference} on {Computer} {Vision} {Workshops}},
	author = {Krause, Jonathan and Stark, Michael and Deng, Jia and Fei-Fei, Li},
	month = dec,
	year = {2013},
	keywords = {Design automation, Feature extraction, Geometry, Solid modeling, Three-dimensional displays, Training, Training data},
	pages = {554--561},
}

@misc{wah_caltech-ucsd_2011,
	type = {Report or {Paper}},
	title = {The {Caltech}-{UCSD} {Birds}-200-2011 {Dataset}},
	copyright = {other},
	url = {https://resolver.caltech.edu/CaltechAUTHORS:20111026-120541847},
	abstract = {CUB-200-2011 is an extended version of CUB-200 [7], a challenging dataset of 200 bird species. The extended version roughly doubles the number of images per category and adds new part localization annotations. All images are annotated with bounding boxes, part locations, and at- tribute labels. Images and annotations were filtered by mul- tiple users of Mechanical Turk. We introduce benchmarks and baseline experiments for multi-class categorization and part localization.},
	language = {en},
	urldate = {2023-06-19},
	author = {Wah, Catherine and Branson, Steve and Welinder, Peter and Perona, Pietro and Belongie, Serge},
	month = jul,
	year = {2011},
}

@misc{noauthor_oml-teamopen-metric-learning_2023,
	title = {{OML}-{Team}/open-metric-learning},
	copyright = {Apache-2.0},
	url = {https://github.com/OML-Team/open-metric-learning},
	abstract = {Library for metric learning pipelines},
	urldate = {2023-06-19},
	publisher = {OML-Team},
	month = jun,
	year = {2023},
	note = {original-date: 2022-06-04T13:12:25Z},
	keywords = {computer-vision, data-science, deep-learning, hacktoberfest, metric-learning, pytorch, pytorch-lightning, representation-learning, similarity-learning},
}

@inproceedings{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	volume = {32},
	shorttitle = {{PyTorch}},
	url = {https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html},
	abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.
In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance.
We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks.},
	urldate = {2023-06-19},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	year = {2019},
}

@misc{dosovitskiy_image_2021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	doi = {10.48550/arXiv.2010.11929},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	urldate = {2023-06-18},
	publisher = {arXiv},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = jun,
	year = {2021},
	note = {arXiv:2010.11929 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{lou_differentiating_2021,
	title = {Differentiating through the {Fr}\'echet {Mean}},
	url = {http://arxiv.org/abs/2003.00335},
	doi = {10.48550/arXiv.2003.00335},
	abstract = {Recent advances in deep representation learning on Riemannian manifolds extend classical deep learning operations to better capture the geometry of the manifold. One possible extension is the Fr\'echet mean, the generalization of the Euclidean mean; however, it has been difficult to apply because it lacks a closed form with an easily computable derivative. In this paper, we show how to differentiate through the Fr\'echet mean for arbitrary Riemannian manifolds. Then, focusing on hyperbolic space, we derive explicit gradient expressions and a fast, accurate, and hyperparameter-free Fr\'echet mean solver. This fully integrates the Fr\'echet mean into the hyperbolic neural network pipeline. To demonstrate this integration, we present two case studies. First, we apply our Fr\'echet mean to the existing Hyperbolic Graph Convolutional Network, replacing its projected aggregation to obtain state-of-the-art results on datasets with high hyperbolicity. Second, to demonstrate the Fr\'echet mean's capacity to generalize Euclidean neural network operations, we develop a hyperbolic batch normalization method that gives an improvement parallel to the one observed in the Euclidean setting.},
	urldate = {2023-06-18},
	publisher = {arXiv},
	author = {Lou, Aaron and Katsman, Isay and Jiang, Qingxuan and Belongie, Serge and Lim, Ser-Nam and De Sa, Christopher},
	month = jul,
	year = {2021},
	note = {arXiv:2003.00335 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{liu_thg_2021,
	title = {{THG}: {Transformer} with {Hyperbolic} {Geometry}},
	shorttitle = {{THG}},
	url = {http://arxiv.org/abs/2106.07350},
	doi = {10.48550/arXiv.2106.07350},
	abstract = {Transformer model architectures have become an indispensable staple in deep learning lately for their effectiveness across a range of tasks. Recently, a surge of "X-former" models have been proposed which improve upon the original Transformer architecture. However, most of these variants make changes only around the quadratic time and memory complexity of self-attention, i.e. the dot product between the query and the key. What's more, they are calculate solely in Euclidean space. In this work, we propose a novel Transformer with Hyperbolic Geometry (THG) model, which take the advantage of both Euclidean space and Hyperbolic space. THG makes improvements in linear transformations of self-attention, which are applied on the input sequence to get the query and the key, with the proposed hyperbolic linear. Extensive experiments on sequence labeling task, machine reading comprehension task and classification task demonstrate the effectiveness and generalizability of our model. It also demonstrates THG could alleviate overfitting.},
	urldate = {2023-06-18},
	publisher = {arXiv},
	author = {Liu, Zhe and Xu, Yibin},
	month = jun,
	year = {2021},
	note = {arXiv:2106.07350 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{kochurov_geoopt_2020,
	title = {Geoopt: {Riemannian} {Optimization} in {PyTorch}},
	shorttitle = {Geoopt},
	url = {http://arxiv.org/abs/2005.02819},
	doi = {10.48550/arXiv.2005.02819},
	abstract = {Geoopt is a research-oriented modular open-source package for Riemannian Optimization in PyTorch. The core of Geoopt is a standard Manifold interface that allows for the generic implementation of optimization algorithms. Geoopt supports basic Riemannian SGD as well as adaptive optimization algorithms. Geoopt also provides several algorithms and arithmetic methods for supported manifolds, which allow composing geometry-aware neural network layers that can be integrated with existing models.},
	urldate = {2023-06-18},
	publisher = {arXiv},
	author = {Kochurov, Max and Karimov, Rasul and Kozlukov, Serge},
	month = jul,
	year = {2020},
	note = {arXiv:2005.02819 [cs]},
	keywords = {53-04, Computer Science - Computational Geometry, Computer Science - Machine Learning, G.4},
}

@misc{becigneul_riemannian_2019,
	title = {Riemannian {Adaptive} {Optimization} {Methods}},
	url = {http://arxiv.org/abs/1810.00760},
	doi = {10.48550/arXiv.1810.00760},
	abstract = {Several first order stochastic optimization methods commonly used in the Euclidean domain such as stochastic gradient descent (SGD), accelerated gradient descent or variance reduced methods have already been adapted to certain Riemannian settings. However, some of the most popular of these optimization tools - namely Adam , Adagrad and the more recent Amsgrad - remain to be generalized to Riemannian manifolds. We discuss the difficulty of generalizing such adaptive schemes to the most agnostic Riemannian setting, and then provide algorithms and convergence proofs for geodesically convex objectives in the particular case of a product of Riemannian manifolds, in which adaptivity is implemented across manifolds in the cartesian product. Our generalization is tight in the sense that choosing the Euclidean space as Riemannian manifold yields the same algorithms and regret bounds as those that were already known for the standard algorithms. Experimentally, we show faster convergence and to a lower train loss value for Riemannian adaptive methods over their corresponding baselines on the realistic task of embedding the WordNet taxonomy in the Poincare ball.},
	urldate = {2023-06-18},
	publisher = {arXiv},
	author = {Bécigneul, Gary and Ganea, Octavian-Eugen},
	month = feb,
	year = {2019},
	note = {arXiv:1810.00760 [cs, stat]
version: 2},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{bonnabel_stochastic_2013,
	title = {Stochastic {Gradient} {Descent} on {Riemannian} {Manifolds}},
	volume = {58},
	issn = {1558-2523},
	doi = {10.1109/TAC.2013.2254619},
	abstract = {Stochastic gradient descent is a simple approach to find the local minima of a cost function whose evaluations are corrupted by noise. In this paper, we develop a procedure extending stochastic gradient descent algorithms to the case where the function is defined on a Riemannian manifold. We prove that, as in the Euclidian case, the gradient descent algorithm converges to a critical point of the cost function. The algorithm has numerous potential applications, and is illustrated here by four examples. In particular a novel gossip algorithm on the set of covariance matrices is derived and tested numerically.},
	number = {9},
	journal = {IEEE Transactions on Automatic Control},
	author = {Bonnabel, Silvère},
	month = sep,
	year = {2013},
	keywords = {Approximation methods, Convergence, Cost function, Covariance matrices, Manifolds, Nonlinear identification, Riemannian geometry, Standards, Trajectory, stochastic approximation},
	pages = {2217--2229},
}

@misc{gulcehre_hyperbolic_2018,
	title = {Hyperbolic {Attention} {Networks}},
	url = {http://arxiv.org/abs/1805.09786},
	doi = {10.48550/arXiv.1805.09786},
	abstract = {We introduce hyperbolic attention networks to endow neural networks with enough capacity to match the complexity of data with hierarchical and power-law structure. A few recent approaches have successfully demonstrated the benefits of imposing hyperbolic geometry on the parameters of shallow networks. We extend this line of work by imposing hyperbolic geometry on the activations of neural networks. This allows us to exploit hyperbolic geometry to reason about embeddings produced by deep networks. We achieve this by re-expressing the ubiquitous mechanism of soft attention in terms of operations defined for hyperboloid and Klein models. Our method shows improvements in terms of generalization on neural machine translation, learning on graphs and visual question answering tasks while keeping the neural representations compact.},
	urldate = {2023-06-18},
	publisher = {arXiv},
	author = {Gulcehre, Caglar and Denil, Misha and Malinowski, Mateusz and Razavi, Ali and Pascanu, Razvan and Hermann, Karl Moritz and Battaglia, Peter and Bapst, Victor and Raposo, David and Santoro, Adam and de Freitas, Nando},
	month = may,
	year = {2018},
	note = {arXiv:1805.09786 [cs]},
	keywords = {Computer Science - Neural and Evolutionary Computing},
}

@article{zhang_hyperbolic_2022,
	title = {Hyperbolic {Graph} {Attention} {Network}},
	volume = {8},
	issn = {2332-7790},
	doi = {10.1109/TBDATA.2021.3081431},
	abstract = {Graph neural network (GNN) has shown superior performance in dealing with structured graphs, which has attracted considerable research attention recently. Most of the existing GNNs are designed in euclidean spaces; however, real-world spatial structured data can be non-euclidean surfaces (e.g., hyperbolic spaces). For example, biologists may inspect the geometric shape of a protein surface to determine its interaction with other biomolecules for drug discovery. Although there is growing research on generalizing GNNs to non-euclidean surfaces, the works in these fields are still scarce. In this article, we exploit the graph attention network to learn robust node representations of graphs in hyperbolic spaces. As the gyrovector space framework provides an elegant algebraic formalism for hyperbolic geometry, we utilize this framework to learn the graph representations in hyperbolic spaces. Specifically, we first use the operations defined in the framework to transform the features in a graph; and we exploit the proximity in the product of hyperbolic spaces to model the multi-head attention mechanism in the non-Euclidean setting; afterward, we further devise a parallel strategy using logarithmic and exponential maps to improve the efficiency of our proposed model. The comprehensive experimental results demonstrate the effectiveness of the proposed model, compared with state-of-the-art methods.},
	number = {6},
	journal = {IEEE Transactions on Big Data},
	author = {Zhang, Yiding and Wang, Xiao and Shi, Chuan and Jiang, Xunqiang and Ye, Yanfang},
	month = dec,
	year = {2022},
	keywords = {Biological system modeling, Convolution, Data models, Deep learning, Geometry, Graph neural networks, Recommender systems, Social networking (online), graph neural network, hyperbolic space, representation learning},
	pages = {1690--1701},
}

@misc{shimizu_hyperbolic_2021,
	title = {Hyperbolic {Neural} {Networks}++},
	url = {http://arxiv.org/abs/2006.08210},
	doi = {10.48550/arXiv.2006.08210},
	abstract = {Hyperbolic spaces, which have the capacity to embed tree structures without distortion owing to their exponential volume growth, have recently been applied to machine learning to better capture the hierarchical nature of data. In this study, we generalize the fundamental components of neural networks in a single hyperbolic geometry model, namely, the Poincar\'e ball model. This novel methodology constructs a multinomial logistic regression, fully-connected layers, convolutional layers, and attention mechanisms under a unified mathematical interpretation, without increasing the parameters. Experiments show the superior parameter efficiency of our methods compared to conventional hyperbolic components, and stability and outperformance over their Euclidean counterparts.},
	urldate = {2023-06-18},
	publisher = {arXiv},
	author = {Shimizu, Ryohei and Mukuta, Yusuke and Harada, Tatsuya},
	month = mar,
	year = {2021},
	note = {arXiv:2006.08210 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{yue_hyperbolic_2023-1,
	title = {Hyperbolic {Contrastive} {Learning}},
	url = {http://arxiv.org/abs/2302.01409},
	doi = {10.48550/arXiv.2302.01409},
	abstract = {Learning good image representations that are beneficial to downstream tasks is a challenging task in computer vision. As such, a wide variety of self-supervised learning approaches have been proposed. Among them, contrastive learning has shown competitive performance on several benchmark datasets. The embeddings of contrastive learning are arranged on a hypersphere that results in using the inner (dot) product as a distance measurement in Euclidean space. However, the underlying structure of many scientific fields like social networks, brain imaging, and computer graphics data exhibit highly non-Euclidean latent geometry. We propose a novel contrastive learning framework to learn semantic relationships in the hyperbolic space. Hyperbolic space is a continuous version of trees that naturally owns the ability to model hierarchical structures and is thus beneficial for efficient contrastive representation learning. We also extend the proposed Hyperbolic Contrastive Learning (HCL) to the supervised domain and studied the adversarial robustness of HCL. The comprehensive experiments show that our proposed method achieves better results on self-supervised pretraining, supervised classification, and higher robust accuracy than baseline methods.},
	urldate = {2023-06-18},
	publisher = {arXiv},
	author = {Yue, Yun and Lin, Fangzhou and Yamada, Kazunori D. and Zhang, Ziming},
	month = feb,
	year = {2023},
	note = {arXiv:2302.01409 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{mishne_numerical_2023,
	title = {The {Numerical} {Stability} of {Hyperbolic} {Representation} {Learning}},
	url = {http://arxiv.org/abs/2211.00181},
	doi = {10.48550/arXiv.2211.00181},
	abstract = {Given the exponential growth of the volume of the ball w.r.t. its radius, the hyperbolic space is capable of embedding trees with arbitrarily small distortion and hence has received wide attention for representing hierarchical datasets. However, this exponential growth property comes at a price of numerical instability such that training hyperbolic learning models will sometimes lead to catastrophic NaN problems, encountering unrepresentable values in floating point arithmetic. In this work, we carefully analyze the limitation of two popular models for the hyperbolic space, namely, the Poincar\'e ball and the Lorentz model. We first show that, under the 64 bit arithmetic system, the Poincar\'e ball has a relatively larger capacity than the Lorentz model for correctly representing points. Then, we theoretically validate the superiority of the Lorentz model over the Poincar\'e ball from the perspective of optimization. Given the numerical limitations of both models, we identify one Euclidean parametrization of the hyperbolic space which can alleviate these limitations. We further extend this Euclidean parametrization to hyperbolic hyperplanes and exhibits its ability in improving the performance of hyperbolic SVM.},
	urldate = {2023-06-18},
	publisher = {arXiv},
	author = {Mishne, Gal and Wan, Zhengchao and Wang, Yusu and Yang, Sheng},
	month = jun,
	year = {2023},
	note = {arXiv:2211.00181 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
}

@misc{chen_hyperbolic_2022,
	title = {Hyperbolic {Uncertainty} {Aware} {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/2203.08881},
	doi = {10.48550/arXiv.2203.08881},
	abstract = {Semantic segmentation (SS) aims to classify each pixel into one of the pre-defined classes. This task plays an important role in self-driving cars and autonomous drones. In SS, many works have shown that most misclassified pixels are commonly near object boundaries with high uncertainties. However, existing SS loss functions are not tailored to handle these uncertain pixels during training, as these pixels are usually treated equally as confidently classified pixels and cannot be embedded with arbitrary low distortion in Euclidean space, thereby degenerating the performance of SS. To overcome this problem, this paper designs a "Hyperbolic Uncertainty Loss" (HyperUL), which dynamically highlights the misclassified and high-uncertainty pixels in Hyperbolic space during training via the hyperbolic distances. The proposed HyperUL is model agnostic and can be easily applied to various neural architectures. After employing HyperUL to three recent SS models, the experimental results on Cityscapes and UAVid datasets reveal that the segmentation performance of existing SS models can be consistently improved.},
	urldate = {2023-06-18},
	publisher = {arXiv},
	author = {Chen, Bike and Peng, Wei and Cao, Xiaofeng and Röning, Juha},
	month = mar,
	year = {2022},
	note = {arXiv:2203.08881 [cs]
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@inproceedings{atigh_hyperbolic_2022,
	title = {Hyperbolic {Image} {Segmentation}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Atigh_Hyperbolic_Image_Segmentation_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-06-18},
	author = {Atigh, Mina Ghadimi and Schoep, Julian and Acar, Erman and van Noord, Nanne and Mettes, Pascal},
	year = {2022},
	pages = {4453--4462},
}

@inproceedings{guo_clipped_2022,
	title = {Clipped {Hyperbolic} {Classifiers} {Are} {Super}-{Hyperbolic} {Classifiers}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Guo_Clipped_Hyperbolic_Classifiers_Are_Super-Hyperbolic_Classifiers_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-06-18},
	author = {Guo, Yunhui and Wang, Xudong and Chen, Yubei and Yu, Stella X.},
	year = {2022},
	pages = {11--20},
}

@inproceedings{ganea_hyperbolic_2018,
	title = {Hyperbolic {Neural} {Networks}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper/2018/hash/dbab2adc8f9d078009ee3fa810bea142-Abstract.html},
	abstract = {Hyperbolic spaces have recently gained momentum in the context of machine learning due to their high capacity and tree-likeliness properties. However, the representational power of hyperbolic geometry is not yet on par with Euclidean geometry, firstly because of the absence of corresponding hyperbolic neural network layers. Here, we bridge this gap in a principled manner by combining the formalism of Möbius gyrovector spaces with the Riemannian geometry of the Poincaré model of hyperbolic spaces. As a result, we derive hyperbolic versions of important deep learning tools: multinomial logistic regression, feed-forward and recurrent neural networks. This allows to embed sequential data and perform classification in the hyperbolic space. Empirically, we show that, even if hyperbolic optimization tools are limited, hyperbolic sentence embeddings either outperform or are on par with their Euclidean variants on textual entailment and noisy-prefix recognition tasks.},
	urldate = {2023-06-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ganea, Octavian and Becigneul, Gary and Hofmann, Thomas},
	year = {2018},
}

@inproceedings{khrulkov_hyperbolic_2020,
	title = {Hyperbolic {Image} {Embeddings}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Khrulkov_Hyperbolic_Image_Embeddings_CVPR_2020_paper.html},
	urldate = {2023-06-18},
	author = {Khrulkov, Valentin and Mirvakhabova, Leyla and Ustinova, Evgeniya and Oseledets, Ivan and Lempitsky, Victor},
	year = {2020},
	pages = {6418--6428},
}

@misc{hermans_defense_2017,
	title = {In {Defense} of the {Triplet} {Loss} for {Person} {Re}-{Identification}},
	url = {http://arxiv.org/abs/1703.07737},
	doi = {10.48550/arXiv.1703.07737},
	abstract = {In the past few years, the field of computer vision has gone through a revolution fueled mainly by the advent of large datasets and the adoption of deep convolutional neural networks for end-to-end learning. The person re-identification subfield is no exception to this. Unfortunately, a prevailing belief in the community seems to be that the triplet loss is inferior to using surrogate losses (classification, verification) followed by a separate metric learning step. We show that, for models trained from scratch as well as pretrained ones, using a variant of the triplet loss to perform end-to-end deep metric learning outperforms most other published methods by a large margin.},
	urldate = {2023-06-18},
	publisher = {arXiv},
	author = {Hermans, Alexander and Beyer, Lucas and Leibe, Bastian},
	month = nov,
	year = {2017},
	note = {arXiv:1703.07737 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{ermolov_hyperbolic_2022,
	title = {Hyperbolic {Vision} {Transformers}: {Combining} {Improvements} in {Metric} {Learning}},
	shorttitle = {Hyperbolic {Vision} {Transformers}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Ermolov_Hyperbolic_Vision_Transformers_Combining_Improvements_in_Metric_Learning_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-06-18},
	author = {Ermolov, Aleksandr and Mirvakhabova, Leyla and Khrulkov, Valentin and Sebe, Nicu and Oseledets, Ivan},
	year = {2022},
	pages = {7409--7419},
}

@inproceedings{touvron_training_2021,
	title = {Training data-efficient image transformers \& distillation through attention},
	url = {https://proceedings.mlr.press/v139/touvron21a.html},
	abstract = {Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. These high-performing vision transformers are pre-trained with hundreds of millions of images using a large infrastructure, thereby limiting their adoption. In this work, we produce competitive convolution-free transformers trained on ImageNet only using a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1\% (single-crop) on ImageNet with no external data. We also introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention, typically from a convnet teacher. The learned transformers are competitive (85.2\% top-1 acc.) with the state of the art on ImageNet, and similarly when transferred to other tasks. We will share our code and models.},
	language = {en},
	urldate = {2023-06-18},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jegou, Herve},
	month = jul,
	year = {2021},
	pages = {10347--10357},
}

@inproceedings{caron_emerging_2021,
	title = {Emerging {Properties} in {Self}-{Supervised} {Vision} {Transformers}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Caron_Emerging_Properties_in_Self-Supervised_Vision_Transformers_ICCV_2021_paper.html},
	language = {en},
	urldate = {2023-06-18},
	author = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and Jégou, Hervé and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
	year = {2021},
	pages = {9650--9660},
}

@misc{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	doi = {10.48550/arXiv.1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2023-06-18},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2023-06-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
}

@misc{oquab_dinov2_2023,
	title = {{DINOv2}: {Learning} {Robust} {Visual} {Features} without {Supervision}},
	shorttitle = {{DINOv2}},
	url = {http://arxiv.org/abs/2304.07193},
	doi = {10.48550/arXiv.2304.07193},
	abstract = {The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.},
	urldate = {2023-06-18},
	publisher = {arXiv},
	author = {Oquab, Maxime and Darcet, Timothée and Moutakanni, Théo and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and Assran, Mahmoud and Ballas, Nicolas and Galuba, Wojciech and Howes, Russell and Huang, Po-Yao and Li, Shang-Wen and Misra, Ishan and Rabbat, Michael and Sharma, Vasu and Synnaeve, Gabriel and Xu, Hu and Jegou, Hervé and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr},
	month = apr,
	year = {2023},
	note = {arXiv:2304.07193 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}
